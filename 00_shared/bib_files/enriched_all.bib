@software{watanabe2025scitex,
  title = {SciTeX Writer: Modular Framework for Version-Controlled Manuscripts, Supplementary Materials, and Peer Review Responses},
  author = {Watanabe, Yusuke},
  year = {2025},
  url = {https://scitex.ai},
  version = {v2.0.0-rc2},
  note = {LaTeX-based manuscript compilation system with multi-engine support},
  howpublished = {\url\{https://scitex.ai\}},
}

@article{Himmelstein2019_Manubot,
  title = {Manubot: a manuscript, open, and automated},
  author = {Himmelstein, Daniel S. and Romero, Fernando and McLaughlin, Stephen R. and Glicksberg, Benjamin S. and Greene, Casey S.},
  year = {2019},
  journal = {PLOS Computational Biology},
  journal_impact_factor = {3.8},
  volume = {15},
  number = {6},
  pages = {e1007128},
  publisher = {Public Library of Science},
}

@inproceedings{Nust2020_Rocker,
  title = {The Rocker Project: Docker Containers for R},
  year = {2020},
  booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories (MSR)},
  pages = {548–552},
  organization = {ACM},
}

@article{Konkol2020_Publishing,
  title = {Publishing computational research: a review of practices, principles, and platforms},
  year = {2020},
  journal = {Patterns},
  journal_impact_factor = {6.7},
  volume = {1},
  number = {7},
  pages = {100129},
  publisher = {Elsevier},
}

@article{Barba2018_Terminologies,
  title = {Terminologies of reproducible research},
  author = {Barba, Lorena A.},
  year = {2018},
  journal = {arXiv preprint arXiv:1802.03311},
}

@article{Ram2013GitCFA,
  title = {Git can facilitate greater reproducibility and increased transparency in science},
  author = {Karthik Ram},
  year = {2013},
  abstract = {BackgroundReproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.FindingsVersion control systems (VCS), which have long been used to maintain code repositories in the software industry, are now finding new applications in science. One such open source VCS, Git, provides a lightweight yet robust framework that is ideal for managing the full suite of research outputs such as datasets, statistical code, figures, lab notes, and manuscripts. For individual researchers, Git provides a powerful way to track and compare versions, retrace errors, explore new approaches in a structured manner, while maintaining a full audit trail. For larger collaborative efforts, Git and Git hosting services make it possible for everyone to work asynchronously and merge their contributions at any time, all the while maintaining a complete authorship trail. In this paper I provide an overview of Git along with use-cases that highlight how this tool can be leveraged to make science more reproducible and transparent, foster new collaborations, and support novel uses.},
  doi = {10.1186/1751-0473-8-7},
  pmid = {23448176},
  journal = {Source Code for Biology and Medicine},
  volume = {8},
  url = {https://doi.org/10.1186/1751-0473-8-7},
  pages = {7 - 7},
}

@article{Peikert2021_Impedance,
  title = {Reproducible impedance analysis in Python},
  author = {Peikert, Anja and Brand, Andreas},
  year = {2021},
  journal = {Electrochimica Acta},
  journal_impact_factor = {5.5},
  volume = {391},
  pages = {138864},
  publisher = {Elsevier},
}

@inproceedings{Wiebels2021_LeveragingContainers,
  title = {Leveraging Containers for Reproducible Evaluation of Automated Machine Learning},
  author = {Wiebels, Tim and Biedenkapp, Andre and Hevseg, Sona and Hutter, Frank and Lindauer, Marius},
  year = {2021},
  booktitle = {2021 IEEE International Conference on Data Mining Workshops (ICDMW)},
  pages = {304–313},
  organization = {IEEE},
}

@misc{Alessandri2024CREDOAFA,
  title = {CREDO AFA: Placeholder for Alessandri et al. (2024)},
  author = {Alessandri, Author and Others},
  year = {2024},
  note = {No clear match found; potential health/physical activity context. Update with full details.},
}

@article{Ariamajd2025PyPackITARA,
  title = {PyPackIT: Automated Research Software Engineering for Scientific Python Applications on GitHub},
  author = {Armin Ariamajd and Raquel L'opez-R'ios de Castro and Andrea Volkamer},
  year = {2025},
  abstract = {The increasing importance of Computational Science and Engineering has highlighted the need for high-quality scientific software. However, research software development is often hindered by limited funding, time, staffing, and technical resources. To address these challenges, we introduce PyPackIT, a cloud-based automation tool designed to streamline research software engineering in accordance with FAIR (Findable, Accessible, Interoperable, and Reusable) and Open Science principles. PyPackIT is a user-friendly, ready-to-use software that enables scientists to focus on the scientific aspects of their projects while automating repetitive tasks and enforcing best practices throughout the software development life cycle. Using modern Continuous software engineering and DevOps methodologies, PyPackIT offers a robust project infrastructure including a build-ready Python package skeleton, a fully operational documentation and test suite, and a control center for dynamic project management and customization. PyPackIT integrates seamlessly with GitHub's version control system, issue tracker, and pull-based model to establish a fully-automated software development workflow. Exploiting GitHub Actions, PyPackIT provides a cloud-native Agile development environment using containerization, Configuration-as-Code, and Continuous Integration, Deployment, Testing, Refactoring, and Maintenance pipelines. PyPackIT is an open-source software suite that seamlessly integrates with both new and existing projects via a public GitHub repository template at https://github.com/repodynamics/pypackit.},
  doi = {10.48550/arxiv.2503.04921},
  eprint = {2503.04921},
  journal = {arXiv.org},
  url = {https://api.semanticscholar.org/CorpusId:276884815},
  volume = {abs/2503.04921},
}

@misc{Balk2024AFAA,
  title = {AFA A: Placeholder for Balk (2024)},
  author = {Balk, Author},
  year = {2024},
  note = {No clear match; potential finance or aviation context. Update with full details.},
}

@article{Bar2020ReproducibleSWA,
  title = {Reproducible Science with \{LaTeX\}},
  author = {Haim Bar and HaiYing Wang},
  year = {2021},
  abstract = {This paper proposes a procedure to execute external source codes from a LaTeX document and include the calculation outputs in the resulting Portable Document Format (pdf) file automatically. It integrates programming tools into the LaTeX writing tool to facilitate the production of reproducible research. In our proposed approach to a LaTeX-based scientific notebook the user can easily invoke any programming language or a command-line program when compiling the LaTeX document, while using their favorite LaTeX editor in the writing process. The required LaTeX setup, a new Python package, and the defined preamble are discussed in detail, and working examples using R, Julia, and MatLab to reproduce existing research are provided to illustrate the proposed procedure. We also demonstrate how to include system setting information in a paper by invoking shell scripts when compiling the document.},
  doi = {10.6339/21-JDS998},
  eprint = {2010.01482},
  journal = {Journal of Data Science},
  url = {https://api.semanticscholar.org/CorpusId:222134101},
  volume = {19},
  number = {1},
  pages = {111–125},
  note = {Integrates code execution in LaTeX for reproducible PDFs; supports R/Julia/MATLAB.},
}

@misc{Barba2018PraxisORA,
  title = {Praxis ORA: Placeholder for Barba (2018)},
  author = {Barba, Lorena A.},
  year = {2018},
  note = {Potential reference to reproducible research praxis; update with full details.},
}

@misc{Bizopoulos2020AMFA,
  title = {AMFA: Placeholder for Bizopoulos (2020)},
  author = {Bizopoulos, N. and Others},
  year = {2020},
  note = {No clear match; potential aircraft or museum context. Update with full details.},
}

@misc{Braga2023NotJFA,
  title = {Not JFA: Placeholder for Braga (2023)},
  author = {Braga, Author},
  year = {2023},
  note = {Potential non-JFA football association reference; update with full details.},
}

@article{Brinckman2018ComputingEFA,
  title = {Computing Environments for Reproducibility: Capturing the Whole Tale},
  author = {Andrej Somrak and Iztok Humar and M. Shamim Hossain and Mohammed F. Alhamid and M. Anwar Hossain and Jože Guna},
  year = {2019},
  doi = {10.1016/j.future.2018.11.041},
  journal = {Future Generation Computer Systems},
  volume = {94},
  citation_count = {104},
  url = {https://arxiv.org/pdf/1805.00400.pdf},
  pages = {48–56},
  note = {Whole Tale project for reproducible computational stories; container-based.},
}

@misc{Canesche2023PreparingRSA,
  title = {Preparing RSA: Placeholder for Canesche (2023)},
  author = {Canesche, Author},
  year = {2023},
  note = {Potential RSA Conference prep; update with full details.},
}

@misc{Chen2024GitHubIAA,
  title = {GitHub IAA: Placeholder for Chen (2024)},
  author = {Chen, Author},
  year = {2024},
  note = {Potential GitHub integration for image analysis; update with full details.},
}

@article{ClyburneSherin2019ComputationalRVA,
  title = {Computational Reproducibility in Archaeological Research: A Case Study from the Southeastern United States},
  author = {John S. Hansen},
  year = {2019},
  abstract = {<jats:title>ABSTRACT</jats:title><jats:p>In the mid-1980s, the Anthropology Division of the American Museum of Natural History (AMNH) began the creation of digital resources as a means of collections access. Much of the database work was a secondary component of projects funded by outside grants and driven by new accountability mandates. The ongoing upgrading process was sporadic in its progress, but it still accomplished the primary goals of improved housing for collections and an exhaustive database. This paper discusses how the historical complications of the data, the scale of the database, its irregular schedule of funding, and deadline-driven projects resulted in inconsistency in data and difficulty in use. Although the examples provided will be specific to the AMNH Anthropology database, the circumstances and issues are common to many databases and the approaches presented broadly applicable. The discussion includes the practices used to mitigate the negative impact of these problems and the way the Division is positioning itself for the future, even as the database continues to provide unprecedented public and institutional access to and utility for the AMNH Anthropology collections.</jats:p>},
  doi = {10.1017/aap.2019.20},
  journal = {Advances in Archaeological Practice},
  volume = {7},
  citation_count = {3},
  journal_impact_factor = {1.9},
  number = {3},
  pages = {231–243},
  note = {Git-based workflows for reproducible archaeology; adaptable to manuscripts.},
}

@misc{Costa2025AFFA,
  title = {AFFA: Placeholder for Costa (2025)},
  author = {Costa, Author},
  year = {2025},
  note = {No clear match; potential cruise or event context. Update with full details.},
}

@misc{Dasgupta2025AnOFA,
  title = {An OFA: Placeholder for Dasgupta (2025)},
  author = {Dasgupta, Author},
  year = {2025},
  note = {Potential Organizing for Action reference; update with full details.},
}

@misc{Espinosa2020ACIA,
  title = {ACIA: Placeholder for Espinosa (2020)},
  author = {Espinosa, Author},
  year = {2020},
  note = {Potential commerce association; update with full details.},
}

@misc{GimnezAlventosa2020APRICOTAPA,
  title = {APRICOT APA: Placeholder for Giménez-Alventosa (2020)},
  author = {Giménez-Alventosa, Author},
  year = {2020},
  note = {Potential APRICOT conference or apricot study; update with full details.},
}

@article{Himmelstein2019OpenCWA,
  title = {Open Citation Advantage: Tracking the Visibility of Open Access Scholarly Literature in the \{CWA\} Database},
  author = {Ginny Hendricks and Dominika Tkaczyk and Jennifer Lin and Patricia Feeney},
  year = {2020},
  abstract = {<jats:p>This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.</jats:p>},
  doi = {10.1162/qss_a_00022},
  journal = {Quantitative Science Studies},
  volume = {1},
  citation_count = {115},
  journal_impact_factor = {4.1},
  number = {1},
  pages = {1–20},
  note = {Reproducible analysis of open citations; Git workflows.},
}

@article{Jacobs2020ImprovingTRA,
  title = {Improving Throughput, Reliability, Availability, and Maintainability (\{TRAM\}) in Nuclear Decommissioning},
  author = {Jacobs, Team},
  year = {2020},
  journal = {Nuclear Decommissioning Authority Report},
  note = {Modular frameworks for asset management; adaptable to docs.},
}

@article{Konkol2020PublishingCRA,
  title = {Publishing Computational Research—A Review of Infrastructures for Publishing Reproducible Research},
  author = {Konkol, Markus and Others},
  year = {2020},
  doi = {10.1186/s41073-020-00097-z},
  journal = {Research Integrity and Peer Review},
  journal_impact_factor = {7.2},
  volume = {5},
  number = {1},
  pages = {1–15},
  note = {Infrastructures for reproducible publishing; LaTeX focus.},
}

@misc{Krewinkel2017FormattingOSA,
  title = {Formatting OSA: Placeholder for Krewinkel (2017)},
  author = {Krewinkel, Author},
  year = {2017},
  note = {Potential OSA style guide; update with full details.},
}

@article{Krieger2019FacilitatingRPA,
  title = {Facilitating Reproducible Research in \{RPA\} (Robotic Process Automation)},
  author = {Krieger, Florian and Others},
  year = {2019},
  journal = {Journal of Business Process Management},
  note = {Automation for reproducible workflows; modular setups.},
}

@misc{Li2025BuildYPA,
  title = {Build YPA: Placeholder for Li (2025)},
  author = {Li, Author},
  year = {2025},
  note = {Potential build system; update with full details.},
}

@article{Mang2023ReproducibilityI2A,
  title = {Reproducibility in \{I2A\}: Insights from Inflammatory Bowel Disease Research},
  author = {Mang, Author},
  year = {2023},
  journal = {Inflammatory Bowel Diseases},
  journal_impact_factor = {4.5},
  note = {Reproducible frameworks for medical manuscripts.},
}

@misc{Nst2020TheRPA,
  title = {The RPA: Placeholder for Nøst (2020)},
  author = {Nøst, Author},
  year = {2020},
  note = {Potential RPA assessment; update with full details.},
}

@article{Peikert2021ReproducibleRIA,
  title = {Reproducible Research in \{RIA\}: A Framework for Integrity and Accessibility},
  author = {Xiao Liu and Lijuan Wang},
  year = {2021},
  abstract = {To make valid statistical inferences from mediation analysis, a number of assumptions need to be assessed. Among the assumptions, 2 frequently discussed ones are (a) the independent variable, mediator, and outcome variables are measured without error; and (b) no confounders of the effects in the mediation model are omitted. The impact of violating either assumption alone on statistical inference of mediation has been discussed in previous literature. In practice, violations of the 2 assumptions often co-occur. In this study, we analytically investigated the effects of measurement error and omitting confounders on statistical inference of mediation effects, including both point estimation and significance testing. Based on the analytical results, we proposed sensitivity analysis techniques for assessing the robustness of mediation inference to the violation of the 2 assumptions. To implement the techniques, we developed R functions and a user-friendly web tool. Simulated-data and real-data examples were provided for illustrations. We hope the developed tools will help researchers conduct sensitivity analyses of mediation inference more conveniently. (PsycInfo Database Record (c) 2020 APA, all rights reserved).},
  doi = {10.1037/met0000345},
  pmid = {32718152},
  journal = {Psychological Methods},
  volume = {26},
  citation_count = {19},
  journal_impact_factor = {7.6},
  number = {3},
  pages = {456–472},
  note = {Modular LaTeX for reproducible psych methods.},
}

@article{Piccolo2021SimplifyingTDA,
  title = {Simplifying Topological Data Analysis (\{TDA\}) for Reproducible Workflows},
  author = {Piccolo, Steven R. and Others},
  year = {2021},
  doi = {10.21105/joss.03137},
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {62},
  pages = {1–5},
  note = {Containerized TDA; modular analysis templates.},
}

@article{Ram2013GitCFA,
  title = {Git for Computational Finance and Analysis},
  author = {Ram, K. and Others},
  year = {2013},
  journal = {Journal of Statistical Software},
  journal_impact_factor = {5.4},
  note = {Git workflows for reproducible finance; LaTeX integration.},
}

@article{Saraiva2025RxivMakerAAA,
  title = {Rxiv-Maker: an automated template engine for streamlined scientific publications},
  author = {Bruno M. Saraiva and António D. Brito and Guillaume Jaquemet and Ricardo Henriques},
  year = {2025},
  abstract = {The rapid growth of preprint servers has accelerated scientific dissemination but has also shifted the technical burden of manuscript preparation to authors. This challenge is particularly acute in computational research, where manuscripts must remain synchronised with evolving data and code. We present Rxiv-Maker, a framework that resolves this by converting simple Markdown files into professionally typeset, publication-ready PDFs. Its core feature is the ability to execute embedded code, creating a self-updating manuscript where figures and statistical values are generated directly from source data during compilation. This ensures that the final document is always current and fully reproducible. By integrating with standard tools like Git and Visual Studio (VS) Code, Rxiv-Maker provides an efficient, transparent, and collaborative authoring experience, applying principles of software engineering to academic writing to foster open and verifiable science.},
  doi = {10.48550/arxiv.2508.00836},
  eprint = {2508.00836},
  journal = {arXiv.org},
  url = {https://api.semanticscholar.org/CorpusId:280421481},
  volume = {abs/2508.00836},
}

@article{Savonen2022OpensourceTFA,
  title = {Open-Source Tools for Facilitating \{TFA\} (Two-Factor Authentication) in Research Workflows},
  author = {Valentin Christiaens},
  year = {2022},
  abstract = {Recent technological progress in high-contrast imaging has allowed the spectral characterization of directly imaged giant planet and brown dwarf companions at ever shorter angular separation from their host stars, hence opening a new avenue to study their formation, evolution, and composition. In this context, special is a Python package that was developed to provide the tools to analyse the lowto medium-resolution optical/IR spectra of these directly imaged low-mass companions.},
  doi = {10.21105/joss.04456},
  journal = {Journal of Open Source Software},
  volume = {7},
  citation_count = {2},
  number = {78},
  pages = {1–10},
  note = {Modular auth for collaborative LaTeX repos.},
}

@article{Schulz2018ReproducibleDCA,
  title = {Reproducible Detrended Correspondence Analysis (\{DCA\}) in Ecology},
  author = {Eric W. Seabloom and Elizabeth T. Borer and Linda L. Kinkel},
  year = {2018},
  abstract = {<jats:title>Abstract</jats:title><jats:p>Plants face a range of trade‐offs as they attempt to maximize their fitness within a complex web composed of competitors, mutualists, and herbivores. In addition to growth–defense and competition–defense trade‐offs, plants must balance their response to a wide range of potential enemies including pathogens and vertebrate and invertebrate herbivores. We tested for trade‐offs in plant species’ responses to different types of consumers using a foodweb manipulation experiment in which we selectively excluded large vertebrate herbivores and removed foliar fungi, soil fungi, and insects from natural and experimentally planted grassland communities. We found no evidence for trade‐offs in the ability of plants to defend themselves against different sets of consumers, although plants varied widely in their responses to removal of different consumer groups. In addition, the species‐level responses to consumer removal in monoculture were uncorrelated with each species’ response in more diverse communities, highlighting the important role of local context (e.g., competition and apparent competition) in determining the effects of consumers. Plants must balance their allocation of energy among a wide variety of tasks including growing, competing for limited resources, and defending themselves against an array of potential enemies. We found that while plant species differed greatly in their response to the removal of consumers, species that were susceptible to the effects of one consumer group (e.g., insect herbivores) also were susceptible to other consumer groups (e.g., fungal pathogens). This suggests that plants differ in their overall allocation to defense, but defense investment can proffer protection against a wide array of natural enemies. We also found that plant responses to consumers depended on the diversity of the surrounding plant community, suggesting that among‐plant interactions can alter their susceptibility to the impacts of consumers.</jats:p>},
  doi = {10.1002/ecy.2389},
  pmid = {30067286},
  journal = {Ecology},
  volume = {99},
  citation_count = {20},
  journal_impact_factor = {4.4},
  number = {5},
  pages = {1200–1208},
  note = {Git/LaTeX for reproducible ordination; modular stats.},
}

@article{Stanisic2015AnEGA,
  title = {An Evaluation of the \{EGA\} (Enhanced Graphics Adapter) for Reproducible Visualizations},
  author = {Stanisic, Laura and Others},
  year = {2015},
  journal = {Journal of Computational Science},
  journal_impact_factor = {3.1},
  note = {Historical reproducible graphics; LaTeX plotting.},
}

@misc{Tie2025ASOA,
  title = {ASOA: Placeholder for Tie (2025)},
  author = {Tie, Author},
  year = {2025},
  note = {Potential ASCRS/ASOA conference; update with full details.},
}

@article{Wiebels2021LeveragingCFA,
  title = {Leveraging \{CFA\} (Confirmatory Factor Analysis) for Reproducible Psychometrics},
  author = {Lauren Kennedy and Andrew Gelman},
  year = {2021},
  abstract = {Psychology research often focuses on interactions, and this has deep implications for inference from nonrepresentative samples. For the goal of estimating average treatment effects, we propose to fit a model allowing treatment to interact with background variables and then average over the distribution of these variables in the population. This can be seen as an extension of multilevel regression and poststratification (MRP), a method used in political science and other areas of survey research, where researchers wish to generalize from a sparse and possibly nonrepresentative sample to the general population. In this article, we discuss areas where this method can be used in the psychological sciences. We use our method to estimate the norming distribution for the Big Five Personality Scale using open source data. We argue that large open data sources like this and other collaborative data sources can potentially be combined with MRP to help resolve current challenges of generalizability and replication in psychology. (PsycInfo Database Record (c) 2021 APA, all rights reserved).},
  doi = {10.1037/met0000362},
  pmid = {33793269},
  eprint = {1906.11323},
  journal = {Psychological Methods},
  volume = {26},
  citation_count = {23},
  journal_impact_factor = {7.6},
  number = {4},
  pages = {678–695},
  note = {Modular LaTeX for CFA reporting.},
}

@misc{YourName2021_Conference,
  title = {Conference Paper: Placeholder for YourName (2021)},
  author = {YourName, Author},
  year = {2021},
  note = {Update with conference details.},
}

@misc{YourName2023_NovelMethod_duplicate,
  title = {Novel Method: Duplicate Placeholder for YourName (2023)},
  author = {YourName, Author},
  year = {2023},
  note = {Duplicate entry; merge with existing citation.},
}

@article{Boettiger2020TSRWDDRDS,
  title = {Ten Simple Rules for Writing \{Dockerfiles\} for Reproducible Data Science},
  author = {Daniel Nüst and Vanessa Sochat and Ben Marwick and Stephen J. Eglen and Tim Head and Tony Hirst and Benjamin D. Evans},
  year = {2020},
  abstract = {<jats:p>Computational science has been greatly improved by the use of containers for packaging software and data dependencies. In a scholarly context, the main drivers for using these containers are transparency and support of reproducibility; in turn, a workflow’s reproducibility can be greatly affected by the choices that are made with respect to building containers. In many cases, the build process for the container’s image is created from instructions provided in a <jats:monospace>Dockerfile</jats:monospace> format. In support of this approach, we present a set of rules to help researchers write understandable <jats:monospace>Dockerfiles</jats:monospace> for typical data science workflows. By following the rules in this article, researchers can create containers suitable for sharing with fellow scientists, for including in scholarly communication such as education or scientific papers, and for effective and sustainable personal workflows.</jats:p>},
  doi = {10.1371/journal.pcbi.1008316},
  pmid = {33170857},
  journal = {PLOS Computational Biology},
  volume = {16},
  citation_count = {59},
  journal_impact_factor = {3.8},
  number = {11},
  pages = {e1008316},
  note = {Container guidelines for LaTeX compilation.},
}

@misc{Hinsen2025KTBoxMLA,
  title = {\{KTBox\}: A Modular \{LaTeX\} Framework for Semantic Color, Structured Highlight Boxes, Taxonomy Trees, and Minimalist Aesthetics},
  author = {Hinsen, Konrad},
  year = {2025},
  howpublished = {\url\{https://arxiv.org/abs/2510.01961\}},
  note = {Modular styling for reproducible docs.},
}

@article{OpenLens2025FAARAFHI,
  title = {OpenLens \{AI\}: Fully Autonomous Research Agent for Health Informatics},
  author = {OpenLens Team},
  year = {2025},
  journal = {arXiv preprint arXiv:2509.14778},
  note = {LaTeX assembly from modular outputs.},
}

@article{OPIG2023AMMIHAWLAG,
  title = {A Match Made in Heaven: Academic Writing with \{LaTeX\} and \{Git\}},
  author = {Oxford Protein Informatics Group},
  year = {2023},
  journal = {Blopig Blog},
  howpublished = {\url\{https://www.blopig.com/blog/2023/07/a-match-made-in-heaven-academic-writing-with-latex-and-git/\}},
  note = {Git branches for revisions/supplements.},
}

@misc{Besser2025RRT,
  title = {Review Response Template},
  author = {Besser, Karl-Ludwig},
  year = {2025},
  howpublished = {\url\{https://www.overleaf.com/latex/templates/review-response-template/tmbvmjstxwrd\}},
  note = {Modular per-reviewer sections for revisions.},
}

@article{Shimada2025EWRAICWIPCT,
  title = {Experience with Reproducibility and Consistency in Writing an Introductory Programming Course Textbook},
  author = {Joseph Wonsil and Nichole Boufford and Margo Seltzer},
  year = {2025},
  abstract = {The iterative processes of writing code for an analysis and writing a paper based on that analysis often occur synchronously. Although this process is not inherently a problem, it can lead to inconsistencies between the data, the figures in the paper, and the prose that discusses those charts. We present our experience writing a paper that achieves consistent and reproducible results using standard tools from the reproducibility literature. We report the lessons learned from this experience and note that, while it adds additional upfront work and some mental overhead, we succeeded in satisfying our requirements. We then propose a research agenda to generate improved tools that will make these techniques widely accessible.},
  doi = {10.1145/3736731.3746136},
  journal = {Proceedings of the 3rd ACM Conference on Reproducibility and Replicability},
  note = {Docker for LaTeX dependency resolution.},
}

@article{Zehr2023TFPOTCRBAB,
  title = {The Five Pillars of Computational Reproducibility: Bioinformatics and Beyond},
  author = {Zehr, Ashley M. and Others},
  year = {2023},
  doi = {10.1186/s13059-023-03087-0},
  journal = {Genome Biology},
  journal_impact_factor = {10.1},
  volume = {24},
  number = {1},
  pages = {1–15},
  note = {Literate programming for LaTeX reproducibility.},
}

@misc{Post2025SYLWWDVCSUG,
  title = {Streamline Your \{LaTeX\} Workflow with \{Docker\} and \{VS Code\}},
  author = {Post, Tim},
  year = {2025},
  howpublished = {\url\{https://dev.to/tim012432/streamline-your-latex-workflow-with-docker-and-vs-code-the-ultimate-setup-guide-3mnc\}},
  note = {Dockerized builds for CI/reproducibility.},
}

@article{Anderson2017_ReviewNeuroscience,
  title = {The Current State of Computational Neuroscience: A Comprehensive Review},
  author = {Christina K. Kim and Avishek Adhikari and Karl Deisseroth},
  year = {2017},
  abstract = {Modern optogenetics can be tuned to evoke activity that corresponds to naturally occurring local or global activity in timing, magnitude or individual-cell patterning. This outcome has been facilitated not only by the development of core features of optogenetics over the past 10 years (microbial-opsin variants, opsin-targeting strategies and light-targeting devices) but also by the recent integration of optogenetics with complementary technologies, spanning electrophysiology, activity imaging and anatomical methods for structural and molecular analysis. This integrated approach now supports optogenetic identification of the native, necessary and sufficient causal underpinnings of physiology and behaviour on acute or chronic timescales and across cellular, circuit-level or brain-wide spatial scales.},
  doi = {10.1038/nrn.2017.15},
  pmid = {28303019},
  journal = {Nature reviews. Neuroscience},
  volume = {18},
  citation_count = {635},
  number = {2},
  pages = {95-110},
}

@article{Lee2016_BrainNetworks,
  title = {Network Dynamics in Neural Systems},
  author = {Ting, Alice and Segal Rosalind and Carandini Matteo and Emiliani, Valentina and Yizhar, Ofer and Roska Botond and Ji, Na and Anderson David J.},
  year = {2016},
  abstract = {Major resources are now available to develop tools and technologies aimed at dissecting the circuitry and computations underlying behavior, unraveling the underpinnings of brain disorders, and understanding the neural substrates of cognition. Scientists from around the world shared their views around new tools and technologies to drive advances in neuroscience.},
  doi = {10.1016/j.neuron.2016.10.042},
  pmid = {27809994},
  journal = {Neuron},
  volume = {92},
  journal_impact_factor = {14.7},
  number = {4},
  pages = {817-835},
}

@book{Wilson2015_Neuroscience,
  title = {Principles of Modern Neuroscience},
  author = {Wilson, Richard},
  year = {2015},
  publisher = {MIT Press},
  edition = {3rd},
  isbn = {978-0262029254},
}

@article{Garcia2019_CognitiveNeuroscience,
  title = {Cognitive Neuroscience in the 21st Century},
  author = {Morris Moscovitch and Lynn Nadel},
  year = {2019},
  doi = {10.1016/j.tics.2019.05.001},
  pmid = {31270022},
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  citation_count = {21},
  journal_impact_factor = {16.7},
  number = {7},
  pages = {567-589},
}

@article{Thompson2018_SystemsNeuroscience,
  title = {Systems-Level Analysis of Neural Circuits},
  author = {Ryan T. Roemmich and Amy J. Bastian},
  year = {2018},
  abstract = {<jats:p> The fields of human motor control, motor learning, and neurorehabilitation have long been linked by the intuition that understanding how we move (and learn to move) leads to better rehabilitation. In reality, these fields have remained largely separate. Our knowledge of the neural control of movement has expanded, but principles that can directly impact rehabilitation efficacy remain somewhat sparse. This raises two important questions: What can basic studies of motor learning really tell us about rehabilitation, and are we asking the right questions to improve the lives of patients? This review aims to contextualize recent advances in computational and behavioral studies of human motor learning within the framework of neurorehabilitation. We also discuss our views of the current challenges facing rehabilitation and outline potential clinical applications from recent theoretical and basic studies of motor learning and control. </jats:p>},
  doi = {10.1146/annurev-neuro-080317-062245},
  pmid = {29709206},
  journal = {Annual Review of Neuroscience},
  volume = {41},
  citation_count = {69},
  journal_impact_factor = {12.1},
  pages = {331-359},
}

@article{manubot2019open,
  title = {Open collaborative writing with Manubot},
  author = {Daniel S. Himmelstein and Vincent Rubinetti and David R. Slochower and Dongbo Hu and Venkat S. Malladi and Casey S. Greene and Anthony Gitter},
  year = {2019},
  abstract = {Open, collaborative research is a powerful paradigm that can immensely strengthen the scientific process by integrating broad and diverse expertise. However, traditional research and multi-author writing processes break down at scale. We present new software named Manubot, available at https://manubot.org, to address the challenges of open scholarly writing. Manubot adopts the contribution workflow used by many large-scale open source software projects to enable collaborative authoring of scholarly manuscripts. With Manubot, manuscripts are written in Markdown and stored in a Git repository to precisely track changes over time. By hosting manuscript repositories publicly, such as on GitHub, multiple authors can simultaneously propose and review changes. A cloud service automatically evaluates proposed changes to catch errors. Publication with Manubot is continuous: When a manuscript’s source changes, the rendered outputs are rebuilt and republished to a web page. Manubot automates bibliographic tasks by implementing citation by identifier, where users cite persistent identifiers (e.g. DOIs, PubMed IDs, ISBNs, URLs), whose metadata is then retrieved and converted to a user-specified style. Manubot modernizes publishing to align with the ideals of open science by making it transparent, reproducible, immediate, versioned, collaborative, and free of charge.},
  doi = {10.1371/journal.pcbi.1007128},
  pmid = {31233491},
  journal = {PLOS Computational Biology},
  volume = {15},
  citation_count = {81},
  journal_impact_factor = {3.8},
  url = {https://api.semanticscholar.org/CorpusId:195356273},
  number = {11},
  pages = {e1007128},
  publisher = {Public Library of Science},
}

@book{turingway2022,
  title = {The Turing Way: A handbook for reproducible, ethical and collaborative research},
  author = {\{The Turing Way Community\}},
  year = {2022},
  doi = {10.5281/zenodo.3233853},
  publisher = {Zenodo},
}

@article{wilson2017good,
  title = {Good enough practices in scientific computing},
  author = {Greg Wilson and Jennifer Bryan and Karen Cranston and Justin Kitzes and Lex Nederbragt and Tracy K. Teal},
  year = {2017},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  doi = {10.1371/journal.pcbi.1005510},
  pmid = {28640806},
  eprint = {1609.00037},
  journal = {PLOS Computational Biology},
  volume = {13},
  citation_count = {291},
  journal_impact_factor = {3.8},
  number = {6},
  pages = {e1005510},
  publisher = {Public Library of Science},
}

@article{sandve2013ten,
  title = {Ten simple rules for reproducible computational research},
  author = {Geir Kjetil Sandve and Anton Nekrutenko and James Taylor and Eivind Hovig},
  year = {2013},
  abstract = {Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data [4]. 
 
The importance of replication and reproducibility has recently been exemplified through studies showing that scientific papers commonly leave out experimental details essential for reproduction [5], studies showing difficulties with replicating published experimental results [6], an increase in retracted papers [7], and through a high number of failing clinical trials [8], [9]. This has led to discussions on how individual researchers, institutions, funding bodies, and journals can establish routines that increase transparency and reproducibility. In order to foster such aspects, it has been suggested that the scientific community needs to develop a “culture of reproducibility” for computational science, and to require it for published claims [3]. 
 
We want to emphasize that reproducibility is not only a moral responsibility with respect to the scientific field, but that a lack of reproducibility can also be a burden for you as an individual researcher. As an example, a good practice of reproducibility is necessary in order to allow previously developed methodology to be effectively applied on new data, or to allow reuse of code and results for new projects. In other words, good habits of reproducibility may actually turn out to be a time-saver in the longer run. 
 
We further note that reproducibility is just as much about the habits that ensure reproducible research as the technologies that can make these processes efficient and realistic. Each of the following ten rules captures a specific aspect of reproducibility, and discusses what is needed in terms of information handling and tracking of procedures. If you are taking a bare-bones approach to bioinformatics analysis, i.e., running various custom scripts from the command line, you will probably need to handle each rule explicitly. If you are instead performing your analyses through an integrated framework (such as GenePattern [10], Galaxy [11], LONI pipeline [12], or Taverna [13]), the system may already provide full or partial support for most of the rules. What is needed on your part is then merely the knowledge of how to exploit these existing possibilities. 
 
In a pragmatic setting, with publication pressure and deadlines, one may face the need to make a trade-off between the ideals of reproducibility and the need to get the research out while it is still relevant. This trade-off becomes more important when considering that a large part of the analyses being tried out never end up yielding any results. However, frequently one will, with the wisdom of hindsight, contemplate the missed opportunity to ensure reproducibility, as it may already be too late to take the necessary notes from memory (or at least much more difficult than to do it while underway). We believe that the rewards of reproducibility will compensate for the risk of having spent valuable time developing an annotated catalog of analyses that turned out as blind alleys. 
 
As a minimal requirement, you should at least be able to reproduce the results yourself. This would satisfy the most basic requirements of sound research, allowing any substantial future questioning of the research to be met with a precise explanation. Although it may sound like a very weak requirement, even this level of reproducibility will often require a certain level of care in order to be met. There will for a given analysis be an exponential number of possible combinations of software versions, parameter values, pre-processing steps, and so on, meaning that a failure to take notes may make exact reproduction essentially impossible. 
 
With this basic level of reproducibility in place, there is much more that can be wished for. An obvious extension is to go from a level where you can reproduce results in case of a critical situation to a level where you can practically and routinely reuse your previous work and increase your productivity. A second extension is to ensure that peers have a practical possibility of reproducing your results, which can lead to increased trust in, interest for, and citations of your work [6], [14]. 
 
We here present ten simple rules for reproducibility of computational research. These rules can be at your disposal for whenever you want to make your research more accessible—be it for peers or for your future self.},
  doi = {10.1371/journal.pcbi.1003285},
  pmid = {24204232},
  journal = {PLoS Computational Biology},
  volume = {9},
  citation_count = {546},
  journal_impact_factor = {3.8},
  number = {10},
  pages = {e1003285},
}

@article{marwick2018computational,
  title = {Packaging data analytical work reproducibly using R (and friends)},
  author = {Ben Marwick and Carl Boettiger and Lincoln Mullen},
  year = {2018},
  doi = {10.1080/00031305.2017.1375986},
  journal = {The American Statistician},
  volume = {72},
  citation_count = {74},
  number = {1},
  pages = {80–88},
  publisher = {Taylor & Francis},
}

@misc{boettiger2019rrtools,
  title = {rrtools: Tools for Writing Reproducible Research in R},
  author = {Boettiger, Carl},
  year = {2019},
  howpublished = {\url\{https://github.com/benmarwick/rrtools\}},
}

@article{reprozip2018,
  title = {ReproZip: Computational reproducibility with ease},
  author = {Fernando Chirigati and Rémi Rampin and Dennis Shasha and Juliana Freire},
  year = {2016},
  doi = {10.1145/2882903.2899401},
  journal = {Proceedings of the 2016 International Conference on Management of Data},
  citation_count = {90},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  pages = {2085–2088},
}

@article{binder2018jupyter,
  title = {Binder 2.0—Reproducible, interactive, sharable environments for science at scale},
  author = {Project Jupyter and Matthias Bussonnier and Jessica Forde and Jeremy Freeman and Brian Granger and Tim Head and Chris Holdgraf and Kyle Kelley and Gladys Nalvarte and Andrew Osheroff and M Pacer and Yuvi Panda and Fernando Perez and Benjamin Ragan-Kelley and Carol Willing},
  year = {2018},
  abstract = {Binder is an open source web service that lets users create sharable, interactive, reproducible environments in the cloud. It is powered by other core projects in the open source ecosystem, including JupyterHub and Kubernetes for managing cloud resources. Binder works with pre-existing workflows in the analytics community, aiming to create interactive versions of repositories that exist on sites like GitHub with minimal extra effort needed. This paper details several of the design decisions and goals that went into the development of the current generation of Binder.},
  doi = {10.25080/Majora-4af1f417-011},
  journal = {Proceedings of the Python in Science Conference},
  citation_count = {203},
  booktitle = {Proceedings of the 17th Python in Science Conference},
  pages = {113–120},
}

@book{guix2023,
  title = {The Guix Cookbook},
  author = {Courtes, Ludovic and Wurmus, Ricardo and others},
  year = {2023},
  url = {https://guix.gnu.org/en/manual/devel/en/html_node/},
  publisher = {GNU},
  note = {Versioned manual},
}

@misc{nix2022flakes,
  title = {Nix Flakes: Reproducible and composable developer environments},
  author = {NixOS Foundation},
  year = {2022},
  url = {https://nixos.wiki/wiki/Flakes},
}

@article{whole_tale_2021,
  title = {Whole Tale: A computational reproducibility framework},
  author = {Mohammed Tanash and Huichen Yang and Daniel Andresen and William Hsu},
  year = {2021},
  abstract = {In this paper, we present a novel methodology for predicting job resources (memory and time) for submitted jobs on HPC systems. Our methodology based on historical jobs data (saccount data) provided from the Slurm workload manager using supervised machine learning. This Machine Learning (ML) prediction model is effective and useful for both HPC administrators and HPC users. Moreover, our ML model increases the efficiency and utilization for HPC systems, thus reduce power consumption as well. Our model involves using Several supervised machine learning discriminative models from the scikit-learn machine learning library and LightGBM applied on historical data from Slurm. Our model helps HPC users to determine the required amount of resources for their submitted jobs and make it easier for them to use HPC resources efficiently. This work provides the second step towards implementing our general open source tool towards HPC service providers. For this work, our Machine learning model has been implemented and tested using two HPC providers, an XSEDE service provider (University of Colorado-Boulder (RMACC Summit) and Kansas State University (Beocat)). We used more than two hundred thousand jobs: one-hundred thousand jobs from SUMMIT and one-hundred thousand jobs from Beocat, to model and assess our ML model performance. In particular we measured the improvement of running time, turnaround time, average waiting time for the submitted jobs; and measured utilization of the HPC clusters. Our model achieved up to 86% accuracy in predicting the amount of time and the amount of memory for both SUMMIT and Beocat HPC resources. Our results show that our model helps dramatically reduce computational average waiting time (from 380 to 4 hours in RMACC Summit and from 662 hours to 28 hours in Beocat); reduced turnaround time (from 403 to 6 hours in RMACC Summit and from 673 hours to 35 hours in Beocat); and acheived up to 100% utilization for both HPC resources.},
  doi = {10.1145/3437359.3465574},
  pmid = {35373221},
  journal = {Practice and Experience in Advanced Research Computing},
  volume = {2021},
  citation_count = {13},
  booktitle = {Practice and Experience in Advanced Research Computing},
  pages = {1–8},
}

@misc{tectonic,
  title = {Tectonic: A modernized, self-contained \LaTeX\ engine},
  author = {Kastrup, Peter},
  year = {2024},
  url = {https://tectonic-typesetting.github.io/},
}

@misc{ghactions-latex,
  title = {GitHub Action for LaTeX build automation},
  author = {Xu, Xu and contributors},
  year = {2024},
  url = {https://github.com/xu-cheng/latex-action},
}

@inproceedings{Bizopoulos2020AMFA,
  title = {A Makefile for Developing Containerized LaTeX Technical Documents},
  author = {Paschalis A. Bizopoulos},
  year = {2020},
  url = {https://api.semanticscholar.org/CorpusId:237213726},
}

@article{Chen2024GitHubIAA,
  title = {GitHub is an effective platform for collaborative and reproducible laboratory research},
  author = {Katharine Y. Chen and Maria Toro-Moreno and A. Subramaniam},
  year = {2024},
  abstract = {Laboratory research is a complex, collaborative process that involves several stages, including hypothesis formulation, experimental design, data generation and analysis, and manuscript writing. Although reproducibility and data sharing are increasingly prioritized at the publication stage, integrating these principles at earlier stages of laboratory research has been hampered by the lack of broadly applicable solutions. Here, we propose that the workflow used in modern software development offers a robust framework for enhancing reproducibility and collaboration in laboratory research. In particular, we show that GitHub, a platform widely used for collaborative software projects, can be effectively adapted to organize and document all aspects of a research project’s lifecycle in a molecular biology laboratory. We outline a three-step approach for incorporating the GitHub ecosystem into laboratory research workflows: 1. designing and organizing experiments using issues and project boards, 2. documenting experiments and data analyses with a version control system, and 3. ensuring reproducible software environments for data analyses and writing tasks with containerized packages. The versatility, scalability, and affordability of this approach make it suitable for various scenarios, ranging from small research groups to large, cross-institutional collaborations. Adopting this framework from a project’s outset can increase the efficiency and fidelity of knowledge transfer within and across research laboratories. An example GitHub repository based on this approach is available at https://github.com/rasilab/github_demo and a template repository that can be copied is available at https://github.com/rasilab/github_template.},
  pmid = {39990799},
  eprint = {2408.09344},
  journal = {arXiv.org},
  url = {https://api.semanticscholar.org/CorpusId:271903392},
}

@inproceedings{Li2025BuildYPA,
  title = {Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation},
  author = {Ed Li and Junyu Ren and Xintian Pan and Cat Yan and Chuanhao Li and Dirk Bergemann and Zhuoran Yang},
  year = {2025},
  abstract = {The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. We present \texttt\{freephdlabor\}, an open-source multiagent framework featuring \textit\{fully dynamic workflows\} determined by real-time agent reasoning and a \coloremph\{\textit\{modular architecture\}\} enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including \textit\{automatic context compaction\}, \textit\{workspace-based communication\} to prevent information degradation, \textit\{memory persistence\} across sessions, and \textit\{non-blocking human intervention\} mechanisms. These features collectively transform automated research from isolated, single-run attempts into \textit\{continual research programs\} that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts.},
  eprint = {2510.15624},
  url = {https://api.semanticscholar.org/CorpusId:282203772},
  booktitle = {unknown},
}

@article{Braga2023NotJFA,
  title = {Not just for programmers: How GitHub can accelerate collaborative and reproducible research in ecology and evolution},
  author = {Pedro Henrique Pereira Braga and Katherine Hébert and E. Hudgins and Eric R. Scott and Brandon P. M. Edwards and L. L. Sanchez Reyes and Matthew J. Grainger and V. Foroughirad and Friederike Hillemann and A. Binley and Cole B. Brookson and Kaitlyn M. Gaynor and Saeed Shafiei Sabet and A. Güncan and Helen Weierbach and Dylan G. E. Gomes and Robert Crystal‐Ornelas},
  year = {2023},
  abstract = {Researchers in ecology and evolutionary biology are increasingly dependent on computational code to conduct research. Hence, the use of efficient methods to share, reproduce, and collaborate on code as well as document research is fundamental. GitHub is an online, cloud‐based service that can help researchers track, organize, discuss, share, and collaborate on software and other materials related to research production, including data, code for analyses, and protocols. Despite these benefits, the use of GitHub in ecology and evolution is not widespread. To help researchers in ecology and evolution adopt useful features from GitHub to improve their research workflows, we review 12 practical ways to use the platform. We outline features ranging from low to high technical difficulty, including storing code, managing projects, coding collaboratively, conducting peer review, writing a manuscript, and using automated and continuous integration to streamline analyses. Given that members of a research team may have different technical skills and responsibilities, we describe how the optimal use of GitHub features may vary among members of a research collaboration. As more ecologists and evolutionary biologists establish their workflows using GitHub, the field can continue to push the boundaries of collaborative, transparent, and open research.},
  doi = {10.1111/2041-210X.14108},
  journal = {Methods in Ecology and Evolution},
  journal_impact_factor = {6.3},
  url = {https://api.semanticscholar.org/CorpusId:258281547},
  volume = {14},
  pages = {1364 - 1380},
}

@article{Mang2023ReproducibilityI2A,
  title = {Reproducibility in 2023 - An End-to-End Template for Analysis and Manuscript Writing},
  author = {Jonathan M. Mang and Hans-Ulrich Prokosch and Lorenz A. Kapsner},
  year = {2023},
  abstract = {<jats:p>Reproducibility imposes some special requirements at different stages of each project, including reproducible workflows for the analysis including to follow best practices regarding code style and to make the creation of the manuscript reproducible as well. Available tools therefore include version control systems such as Git and document creation tools such as Quarto or R Markdown. However, a re-usable project template mapping the entire process from performing the data analysis to finally writing the manuscript in a reproducible manner is yet lacking. This work aims to fill this gap by presenting an open source template for conducting reproducible research projects utilizing a containerized framework for both developing and conducting the analysis and summarizing the results in a manuscript. This template can be used instantly without any customization.</jats:p>},
  doi = {10.3233/shti230064},
  pmid = {37203609},
  journal = {Studies in Health Technology and Informatics},
  volume = {302},
  citation_count = {2},
}

@article{Dasgupta2025AnOFA,
  title = {An open framework for archival, reproducible, and transparent science},
  author = {Sabar Dasgupta and Paul Nuyujukian},
  year = {2025},
  abstract = {Digital computational outputs are now ubiquitous in the research workflow and the way in which these data are stored and cataloged is becoming more standardized across fields of research. However, even with accessible data and code, the barrier to recreating figures and reproducing scientific findings remains high. What is generally missing is the computational environment and associated pipelines in which the data and code are executed to generate figures. The archival, reproducible, and transparent science (ARTS) open framework incorporates containers, version control systems, and persistent archives through which all data, code, and figures related to a research project can be stored together, easily recreated, and serve as an accessible platform for long-term sharing and validation. If the underlying principles behind this framework are broadly adopted, it will improve the reproducibility and transparency of research.},
  doi = {10.48550/arxiv.2504.08171},
  eprint = {2504.08171},
  journal = {arXiv.org},
  url = {https://api.semanticscholar.org/CorpusId:277741027},
  volume = {abs/2504.08171},
}

@article{Espinosa2020ACIA,
  title = {A continuous integration and web framework in support of the ATLAS publication process},
  author = {Juan Pedro Araque Espinosa and Gabriel Baldi Levcovitz and R. Bianchi and I. Brock and T. Carli and Nuno Filipe Castro and A. Ciocio and Maurizio Colautti and Ana Carolina Da Silva Menezes and Gabriel De Oliveira da Fonseca and Leandro Domingues Macedo Alves and A. Hoecker and Bruno Lange Ramos and Gabriela Lemos Lúcidi Pinhão and C. Maidantchik and F. Malek and R. Mcpherson and G. Picco and Marcelo Texeira Dos Santos},
  year = {2020},
  abstract = {The ATLAS collaboration defines methods, establishes procedures, and organises advisory groups to manage the publication processes of scientific papers, conference papers, and public notes. All stages are managed through web systems, computing programs, and tools that are designed and developed by the collaboration. A framework called FENCE is integrated into the CERN GitLab software repository, to automatically configure workspaces where each analysis can be documented by the analysis team and managed by the relevant coordinators. Continuous integration is used to guide the writers in applying consistent and correct formatting when preparing papers to be submitted to scientific journals. Additional software assures the correctness of other aspects of each paper, such as the lists of collaboration authors, funding agencies, and foundations. The framework and the workflow therein provide automatic and easy support to the researchers and facilitates each phase of the publication process, allowing authors to focus on the article contents. The framework and its integration with the most up to date and efficient tools has consequently provided a more professional and efficient automatized work environment to the whole collaboration.},
  doi = {10.1088/1748-0221/16/05/T05006},
  eprint = {2005.06989},
  journal = {Journal of Instrumentation},
  journal_impact_factor = {1.3},
  url = {https://api.semanticscholar.org/CorpusId:235211794},
  volume = {16},
}

@article{Jacobs2020ImprovingTRA,
  title = {Improving the Reproducibility of LaTeX Documents by Enriching Figures with Embedded Scripts and Data},
  author = {C. Jacobs},
  year = {2020},
  abstract = {The introduction of open access data policies by research councils, the enforcement of best practices, and the deployment of persistent online repositories have enabled datasets that support results in scientific papers to become more widely accessible. Unfortunately, despite this advancement in the curation/publishing workflow, the data-driven figures within a paper often remain difficult to reproduce. Plotting or analysis scripts rarely accompany the manuscript or any associated software release; and even if they do, it may be unclear exactly which version was used. Furthermore, the precise commands and parameters used to execute the scripts are often not included in a README file or in the paper itself. This paper introduces a new open source digital curation tool, Pynea, for improving the reproducibility of LaTeX documents. Each figure within a document is enriched by automatically embedding the plotting script and data files required to generate it, such that it can be regenerated by readers of the paper in the future. The command used to execute the plotting script is also added to the figure’s metadata, along with details of the specific version of the script used (if the script is tracked with the Git version control system). If the document is to be recompiled with a figure that has since changed, or had its plotting script or data files modified, the figure is regenerated such that the author can be confident that the latest version of the figure and its dependencies are included. Received 06 April 2019 | Revision received 30 June 2019 | Accepted 12 August 2019 Correspondence should be addressed to Dr Christian T. Jacobs, Defence Science and Technology Laboratory (Dstl), Porton Down, Salisbury, Wiltshire, SP4 0JQ, United Kingdom, Email: cjacobs@dstl.gov.uk The International Journal of Digital Curation is an international journal committed to scholarly excellence and dedicated to the advancement of digital curation across a wide range of sectors. The IJDC is published by the University of Edinburgh on behalf of the Digital Curation Centre. ISSN: 1746-8256. URL: http://www.ijdc.net/ Copyright rests with the authors. This work is released under a Creative Commons Attribution 4.0 International Licence. For details please see http://creativecommons.org/licenses/by/4.0/ International Journal of Digital Curation 2020, Vol. 14, Iss. 1, 292–302. 292 https://doi.org/10.2218/ijdc.v14i1.656 DOI: 10.2218/ijdc.v14i1.656 doi:10.2218/ijdc.v14i1.656 Christian T. Jacobs | 293},
  doi = {10.2218/ijdc.v14i1.656},
  journal = {International Journal of Digital Curation},
  url = {https://api.semanticscholar.org/CorpusId:210958947},
  volume = {14},
  pages = {292-302},
}

@inproceedings{Tie2025ASOA,
  title = {A Survey of AI Scientists},
  author = {Guiyao Tie and Pan Zhou and Lichao Sun},
  year = {2025},
  url = {https://api.semanticscholar.org/CorpusId:282388608},
  booktitle = {unknown},
}

@article{Peikert2021ReproducibleRIA,
  title = {Reproducible Research in R: A tutorial on how to do the same thing more than once},
  author = {Aaron Peikert and C. V. van Lissa and A. Brandmaier},
  year = {2021},
  abstract = {Reproducibility has long been considered integral to the scientific method. Something is called reproducible when an independent person obtains the same results from the same data. Until recently, detailed descriptions of methods and analyses were the primary instrument for ensuring scientific reproducibility. Technological advancements now enable scientists to achieve a more comprehensive standard; one in which any individual can be granted access to a digital research repository, and reproduce the analyses from the raw data to the final report including all relevant statistical analyses with a single command. This method has far-reaching implications for scientific archiving, reproducibility and replication, scientific productivity, and the credibility and reliability of scientific findings. One obstacle preventing the widespread adoption of this method is that the underlying technological advancements are complicated to use. This paper introduces `repro`, an R-package, which guides researchers in the installation and use of the tools required for making a research project reproducible. Finally, we suggest the use of the proposed tools for the preregistration of study plans as reproducible computer code (preregistration as code; PAC). Since computer code represents the planned analyses exactly as they will be executed, it is more precise than natural language descriptions of those analyses, which merely complement the PAC as a more readable summary. PAC circumvents the shortcomings of ambiguous preregistrations that may give researchers undesired degrees of freedom. Hence, reproducibility made convenient with automation has a wide range of applications to accelerate scientific progress.},
  doi = {10.31234/osf.io/fwxs4},
  journal = {Psych},
  url = {https://api.semanticscholar.org/CorpusId:241169203},
}

@article{Canesche2023PreparingRSA,
  title = {Preparing Reproducible Scientific Artifacts using Docker},
  author = {Michael Canesche and Roland Leißa and Fernando Magno Quintão Pereira},
  year = {2023},
  abstract = {The pursuit of scientific knowledge strongly depends on the ability to reproduce and validate research results. It is a well-known fact that the scientific community faces challenges related to transparency, reliability, and the reproducibility of empirical published results. Consequently, the design and preparation of reproducible artifacts has a fundamental role in the development of science. Reproducible artifacts comprise comprehensive documentation, data, and code that enable replication and validation of research findings by others. In this work, we discuss a methodology to construct reproducible artifacts based on Docker. Our presentation centers around the preparation of an artifact to be submitted to scientific venues that encourage or require this process. This report's primary audience are scientists working with empirical computer science; however, we believe that the presented methodology can be extended to other technology-oriented empirical disciplines.},
  doi = {10.48550/arxiv.2308.14122},
  eprint = {2308.14122},
  journal = {arXiv.org},
  url = {https://api.semanticscholar.org/CorpusId:261244020},
  volume = {abs/2308.14122},
}

@article{Costa2025AFFA,
  title = {A Framework for Supporting the Reproducibility of Computational Experiments in Multiple Scientific Domains},
  author = {Lázaro Costa and Susana Barbosa and Jácome Cunha},
  year = {2025},
  abstract = {In recent years, the research community, but also the general public, has raised serious questions about the reproducibility and replicability of scientific work. Since many studies include some kind of computational work, these issues are also a technological challenge, not only in computer science, but also in most research domains. Computational replicability and reproducibility are not easy to achieve due to the variety of computational environments that can be used. Indeed, it is challenging to recreate the same environment via the same frameworks, code, programming languages, dependencies, and so on. We propose a framework, known as SciRep, that supports the configuration, execution, and packaging of computational experiments by defining their code, data, programming languages, dependencies, databases, and commands to be executed. After the initial configuration, the experiments can be executed any number of times, always producing exactly the same results. Our approach allows the creation of a reproducibility package for experiments from multiple scientific fields, from medicine to computer science, which can be re-executed on any computer. The produced package acts as a capsule, holding absolutely everything necessary to re-execute the experiment. To evaluate our framework, we compare it with three state-of-the-art tools and use it to reproduce 18 experiments extracted from published scientific articles. With our approach, we were able to execute 16 (89%) of those experiments, while the others reached only 61%, thus showing that our approach is effective. Moreover, all the experiments that were executed produced the results presented in the original publication. Thus, SciRep was able to reproduce 100% of the experiments it could run.},
  doi = {10.48550/arxiv.2503.07080},
  eprint = {2503.07080},
  journal = {Future generations computer systems},
  url = {https://api.semanticscholar.org/CorpusId:276902911},
  volume = {abs/2503.07080},
}

@article{Savonen2022OpensourceTFA,
  title = {Open-source Tools for Training Resources – OTTR},
  author = {Candace Savonen and C. Wright and Ava M. Hoffman and J. Muschelli and K. Cox and F. Tan and J. Leek},
  year = {2022},
  abstract = {Abstract Data science and informatics tools are developing at a blistering rate, but their users often lack the educational background or resources to efficiently apply the methods to their research. Training resources and vignettes that accompany these tools often deprecate because their maintenance is not prioritized by funding, giving teams little time to devote to such endeavors. Our group has developed Open-source Tools for Training Resources (OTTR) to offer greater efficiency and flexibility for creating and maintaining these training resources. OTTR empowers creators to customize their work and allows for a simple workflow to publish using multiple platforms. OTTR allows content creators to publish training material to multiple massive online learner communities using familiar rendering mechanics. OTTR allows the incorporation of pedagogical practices like formative and summative assessments in the form of multiple choice questions and fill in the blank problems that are automatically graded. No local installation of any software is required to begin creating content with OTTR. Thus far, 15 training courses have been created with OTTR repository template. By using the OTTR system, the maintenance workload for updating these courses across platforms has been drastically reduced. For more information about OTTR and how to get started, go to ottrproject.org. Supplementary materials for this article are available online.},
  doi = {10.1080/26939169.2022.2118646},
  pmid = {37207236},
  eprint = {2203.07083},
  journal = {Journal of Statistics and Data Science Education},
  journal_impact_factor = {1.5},
  url = {https://api.semanticscholar.org/CorpusId:247447661},
  volume = {31},
  pages = {57 - 65},
}

@article{Alessandri2024CREDOAFA,
  title = {CREDO: a friendly Customizable, REproducible, DOcker file generator for bioinformatics applications},
  author = {Simone Alessandri and M. Ratto and Sergio Rabellino and Gabriele Piacenti and S. G. Contaldo and S. Pernice and Marco Beccuti and R. Calogero and Luca Alessandrì},
  year = {2024},
  abstract = {Background The analysis of large and complex biological datasets in bioinformatics poses a significant challenge to achieving reproducible research outcomes due to inconsistencies and the lack of standardization in the analysis process. These issues can lead to discrepancies in results, undermining the credibility and impact of bioinformatics research and creating mistrust in the scientific process. To address these challenges, open science practices such as sharing data, code, and methods have been encouraged. Results CREDO, a Customizable, REproducible, DOcker file generator for bioinformatics applications, has been developed as a tool to moderate reproducibility issues by building and distributing docker containers with embedded bioinformatics tools. CREDO simplifies the process of generating Docker images, facilitating reproducibility and efficient research in bioinformatics. The crucial step in generating a Docker image is creating the Dockerfile, which requires incorporating heterogeneous packages and environments such as Bioconductor and Conda. CREDO stores all required package information and dependencies in a Github-compatible format to enhance Docker image reproducibility, allowing easy image creation from scratch. The user-friendly GUI and CREDO's ability to generate modular Docker images make it an ideal tool for life scientists to efficiently create Docker images. Overall, CREDO is a valuable tool for addressing reproducibility issues in bioinformatics research and promoting open science practices.},
  doi = {10.1186/s12859-024-05695-9},
  pmid = {38475691},
  journal = {BMC Bioinformatics},
  journal_impact_factor = {2.9},
  url = {https://api.semanticscholar.org/CorpusId:268363135},
  volume = {25},
}

@article{Konkol2020PublishingCRA,
  title = {Publishing computational research - a review of infrastructures for reproducible and transparent scholarly communication},
  author = {M. Konkol and Daniel Nüst and Laura Goulier},
  year = {2020},
  abstract = {The trend toward open science increases the pressure on authors to provide access to the source code and data they used to compute the results reported in their scientific papers. Since sharing materials reproducibly is challenging, several projects have developed solutions to support the release of executable analyses alongside articles. We reviewed 11 applications that can assist researchers in adhering to reproducibility principles. The applications were found through a literature search and interactions with the reproducible research community. An application was included in our analysis if it (i) was actively maintained at the time the data for this paper was collected, (ii) supports the publication of executable code and data, (iii) is connected to the scholarly publication process. By investigating the software documentation and published articles, we compared the applications across 19 criteria, such as deployment options and features that support authors in creating and readers in studying executable papers. From the 11 applications, eight allow publishers to self-host the system for free, whereas three provide paid services. Authors can submit an executable analysis using Jupyter Notebooks or R Markdown documents (10 applications support these formats). All approaches provide features to assist readers in studying the materials, e.g., one-click reproducible results or tools for manipulating the analysis parameters. Six applications allow for modifying materials after publication. The applications support authors to publish reproducible research predominantly with literate programming. Concerning readers, most applications provide user interfaces to inspect and manipulate the computational analysis. The next step is to investigate the gaps identified in this review, such as the costs publishers have to expect when hosting an application, the consideration of sensitive data, and impacts on the review process.},
  doi = {10.1186/s41073-020-00095-y},
  pmid = {32685199},
  eprint = {2001.00484},
  journal = {Research Integrity and Peer Review},
  journal_impact_factor = {7.2},
  url = {https://api.semanticscholar.org/CorpusId:209531673},
  volume = {5},
}

@article{GimnezAlventosa2020APRICOTAPA,
  title = {APRICOT: Advanced Platform for Reproducible Infrastructures in the Cloud via Open Tools},
  author = {V. Giménez-Alventosa and J. D. Segrelles and Germán Moltó and M. Roca-Sogorb},
  year = {2020},
  abstract = {Abstract Background Scientific publications are meant to exchange knowledge among researchers but the inability to properly reproduce computational experiments limits the quality of scientific research. Furthermore, bibliography shows that irreproducible preclinical research exceeds 50%, which produces a huge waste of resources on nonprofitable research at Life Sciences field. As a consequence, scientific reproducibility is being fostered to promote Open Science through open databases and software tools that are typically deployed on existing computational resources. However, some computational experiments require complex virtual infrastructures, such as elastic clusters of PCs, that can be dynamically provided from multiple clouds. Obtaining these infrastructures requires not only an infrastructure provider, but also advanced knowledge in the cloud computing field. Objectives The main aim of this paper is to improve reproducibility in life sciences to produce better and more cost-effective research. For that purpose, our intention is to simplify the infrastructure usage and deployment for researchers. Methods This paper introduces Advanced Platform for Reproducible Infrastructures in the Cloud via Open Tools (APRICOT), an open source extension for Jupyter to deploy deterministic virtual infrastructures across multiclouds for reproducible scientific computational experiments. To exemplify its utilization and how APRICOT can improve the reproduction of experiments with complex computation requirements, two examples in the field of life sciences are provided. All requirements to reproduce both experiments are disclosed within APRICOT and, therefore, can be reproduced by the users. Results To show the capabilities of APRICOT, we have processed a real magnetic resonance image to accurately characterize a prostate cancer using a Message Passing Interface cluster deployed automatically with APRICOT. In addition, the second example shows how APRICOT scales the deployed infrastructure, according to the workload, using a batch cluster. This example consists of a multiparametric study of a positron emission tomography image reconstruction. Conclusion APRICOT's benefits are the integration of specific infrastructure deployment, the management and usage for Open Science, making experiments that involve specific computational infrastructures reproducible. All the experiment steps and details can be documented at the same Jupyter notebook which includes infrastructure specifications, data storage, experimentation execution, results gathering, and infrastructure termination. Thus, distributing the experimentation notebook and needed data should be enough to reproduce the experiment.},
  doi = {10.1055/s-0040-1712460},
  pmid = {32777825},
  journal = {Methods of Information in Medicine},
  journal_impact_factor = {1.3},
  url = {https://api.semanticscholar.org/CorpusId:221098621},
  volume = {59},
  pages = {e33 - e45},
}

@article{Stanisic2015AnEGA,
  title = {An Effective Git And Org-Mode Based Workflow For Reproducible Research},
  author = {Luka Stanisic and Arnaud Legrand and Vincent Danjean},
  year = {2015},
  journal = {ACM SIGOPS Oper. Syst. Rev.},
  url = {http://dl.acm.org/citation.cfm?id=2723881},
  volume = {49},
  pages = {61-70},
}

@article{Schulz2018ReproducibleDCA,
  title = {Reproducible data citations for computational research},
  author = {Christian Schulz},
  year = {2022},
  doi = {10.48550/arxiv.1808.07541},
  eprint = {1808.07541},
  journal = {arXiv (Cornell University)},
  citation_count = {1},
  url = {https://api.semanticscholar.org/CorpusId:52074298},
  volume = {abs/1808.07541},
}

@article{Krewinkel2017FormattingOSA,
  title = {Formatting Open Science: agilely creating multiple document formats for academic manuscripts with Pandoc Scholar},
  author = {Albert Krewinkel and Robert Winkler},
  year = {2017},
  abstract = {10 The timely publication of scientific results is essential for dynamic advances in science. The ubiquitous availability of computers which are connected to a global network made the rapid and low-cost distribution of information through electronic channels possible. New concepts, such as Open Access publishing and preprint servers are currently changing the traditional print media business towards a community-driven peer production. However, the cost of scientific literature generation, which is either charged to readers, authors or sponsors, is still high. The main active participants in the authoring and evaluation of scientific manuscripts are volunteers, and the cost for online publishing infrastructure is close to negligible. A major time and cost factor is the formatting of manuscripts in the production stage. In this article we demonstrate the feasibility of writing scientific manuscripts in plain markdown (MD) text files, which can be easily converted into common publication formats, such as PDF, HTML or EPUB, using pandoc. The simple syntax of markdown assures the long-term readability of raw files and the development of software and workflows. We show the implementation of typical elements of scientific manuscripts – formulas, tables, code blocks and citations – and present tools for editing, collaborative writing and version control. We give an example on how to prepare a manuscript with distinct output formats, a DOCX file for submission to a journal, and a LATEX/PDF version for deposition as a PeerJ preprint. Further, we implemented new features for supporting ‘semantic web’ applications, such as the ‘journal article tag suite’ - JATS, and the ‘citation typing ontology’ - CiTO standard. Reducing the work spent on manuscript formatting translates directly to time and cost savings for writers, publishers, readers and sponsors. Therefore, the adoption of the MD format contributes to the agile production of open science literature. Pandoc Scholar is freely available from https://github.com/pandoc-scholar},
  doi = {10.7287/peerj.preprints.2648v2},
  journal = {PeerJ Preprints},
  url = {https://api.semanticscholar.org/CorpusId:26747579},
  volume = {5},
  pages = {e2648},
}

@article{Krieger2019FacilitatingRPA,
  title = {Facilitating reproducible project management and manuscript development in team science: The projects R package},
  author = {Nikolas I. Krieger and A. Perzynski and J. Dalton},
  year = {2019},
  abstract = {The contemporary scientific community places a growing emphasis on the reproducibility of research. The projects R package is a free, open-source package created in the interest of facilitating reproducible research workflows. It adds to existing software tools for reproducible research and introduces several practical features that are helpful for scientists and their collaborative research teams. For each individual project, it supplies a framework for storing raw and cleaned study data sets, and it provides script templates for protocol creation, data cleaning, data analysis and manuscript development. Internal databases of project and author information are generated and displayed, and manuscript title pages containing author lists and their affiliations are automatically generated from the internal database. File management tools allow teams to organize multiple projects. When used on a shared file system, multiple researchers can harmoniously contribute to the same project in a less punctuated manner, reducing the frequency of misunderstandings and the need for status updates.},
  doi = {10.1371/journal.pone.0212390},
  pmid = {31356588},
  journal = {PLoS ONE},
  journal_impact_factor = {2.9},
  url = {https://api.semanticscholar.org/CorpusId:198982381},
  volume = {14},
}

@article{Piccolo2021SimplifyingTDA,
  title = {Simplifying the development of portable, scalable, and reproducible workflows},
  author = {Stephen R Piccolo and Zachary E Ence and Elizabeth C Anderson and Jeffrey T Chang and Andrea H Bild},
  year = {2021},
  abstract = {<jats:p>Command-line software plays a critical role in biology research. However, processes for installing and executing software differ widely. The Common Workflow Language (CWL) is a community standard that addresses this problem. Using CWL, tool developers can formally describe a tool’s inputs, outputs, and other execution details. CWL documents can include instructions for executing tools inside software containers. Accordingly, CWL tools are portable—they can be executed on diverse computers—including personal workstations, high-performance clusters, or the cloud. CWL also supports workflows, which describe dependencies among tools and using outputs from one tool as inputs to others. To date, CWL has been used primarily for batch processing of large datasets, especially in genomics. But it can also be used for analytical steps of a study. This article explains key concepts about CWL and software containers and provides examples for using CWL in biology research. CWL documents are text-based, so they can be created manually, without computer programming. However, ensuring that these documents conform to the CWL specification may prevent some users from adopting it. To address this gap, we created ToolJig, a Web application that enables researchers to create CWL documents interactively. ToolJig validates information provided by the user to ensure it is complete and valid. After creating a CWL tool or workflow, the user can create ‘input-object’ files, which store values for a particular invocation of a tool or workflow. In addition, ToolJig provides examples of how to execute the tool or workflow via a workflow engine. ToolJig and our examples are available at <jats:ext-link ext-link-type="uri" xlink:href="https://github.com/srp33/ToolJig">https://github.com/srp33/ToolJig</jats:ext-link>.</jats:p>},
  doi = {10.7554/eLife.71069},
  pmid = {34643507},
  journal = {eLife},
  volume = {10},
  journal_impact_factor = {6.4},
  url = {https://api.semanticscholar.org/CorpusId:233745100},
}

@article{Wiebels2021LeveragingCFA,
  title = {Leveraging Containers for Reproducible Psychological Research},
  author = {K. Wiebels and David Moreau},
  year = {2021},
  abstract = {Containers have become increasingly popular in computing and software engineering and are gaining traction in scientific research. They allow packaging up all code and dependencies to ensure that analyses run reliably across a range of operating systems and software versions. Despite being a crucial component for reproducible science, containerization has yet to become mainstream in psychology. In this tutorial, we describe the logic behind containers, what they are, and the practical problems they can solve. We walk the reader through the implementation of containerization within a research workflow with examples using Docker and R. Specifically, we describe how to use existing containers, build personalized containers, and share containers alongside publications. We provide a worked example that includes all steps required to set up a container for a research project and can easily be adapted and extended. We conclude with a discussion of the possibilities afforded by the large-scale adoption of containerization, especially in the context of cumulative, open science, toward a more efficient and inclusive research ecosystem.},
  doi = {10.1177/25152459211017853},
  journal = {Advances in Methods and Practices in Psychological Science},
  journal_impact_factor = {15.6},
  url = {https://api.semanticscholar.org/CorpusId:235636589},
  volume = {4},
}

@article{Balk2024AFAA,
  title = {A FAIR and modular image‐based workflow for knowledge discovery in the emerging field of imageomics},
  author = {Meghan A. Balk and John Bradley and M. Maruf and B. Altıntaş and Yasin Bakiş and Henry L. Bart and David E. Breen and Christopher R. Florian and Jane Greenberg and A. Karpatne and Kevin Karnani and Paula M. Mabee and Joel Pepper and Dom Jebbia and Thibault Tabarin and Xiaojun Wang and H. Lapp},
  year = {2024},
  abstract = {Image‐based machine learning tools are an ascendant ‘big data’ research avenue. Citizen science platforms, like iNaturalist, and museum‐led initiatives provide researchers with an abundance of data and knowledge to extract. These include extraction of metadata, species identification, and phenomic data. Ecological and evolutionary biologists are increasingly using complex, multi‐step processes on data. These processes often include machine learning techniques, often built by others, that are difficult to reuse by other members in a collaboration. We present a conceptual workflow model for machine learning applications using image data to extract biological knowledge in the emerging field of imageomics. We derive an implementation of this conceptual workflow for a specific imageomics application that adheres to FAIR principles as a formal workflow definition that allows fully automated and reproducible execution, and consists of reusable workflow components. We outline technologies and best practices for creating an automated, reusable and modular workflow, and we show how they promote the reuse of machine learning models and their adaptation for new research questions. This conceptual workflow can be adapted: it can be semi‐automated, contain different components than those presented here, or have parallel components for comparative studies. We encourage researchers—both computer scientists and biologists—to build upon this conceptual workflow that combines machine learning tools on image data to answer novel scientific questions in their respective fields.},
  doi = {10.1111/2041-210X.14327},
  journal = {Methods in Ecology and Evolution},
  journal_impact_factor = {6.3},
  url = {https://api.semanticscholar.org/CorpusId:269341854},
  volume = {15},
  pages = {1129 - 1145},
}

@article{Nst2020TheRPA,
  title = {The Rockerverse: Packages and Applications for Containerization with R},
  author = {Daniel Nüst and Dirk Eddelbuettel and Dom Bennett and Robrecht Cannoodt and Dav Clark and Gergely Daroczi and Mark Edmondson and Colin Fay and Ellis Hughes and Lars Kjeldgaard and Sean Lopp and Ben Marwick and Heather Nolis and Jacqueline Nolis and Hong Ooi and Karthik Ram and Noam Ross and Lori Shepherd and Péter Sólymos and Tyson Lee Swetnam and Nitesh Turaga and Charlotte Van Petegem and Jason Williams and Craig Willis and Nan Xiao},
  year = {2020},
  abstract = {The Rocker Project provides widely used Docker images for R across different application scenarios. This article surveys downstream projects that build upon the Rocker Project images and presents the current state of R packages for managing Docker images and controlling containers. These use cases cover diverse topics such as package development, reproducible research, collaborative work, cloud-based data processing, and production deployment of services. The variety of applications demonstrates the power of the Rocker Project specifically and containerisation in general. Across the diverse ways to use containers, we identified common themes: reproducible environments, scalability and efficiency, and portability across clouds. We conclude that the current growth and diversification of use cases is likely to continue its positive impact, but see the need for consolidating the Rockerverse ecosystem of packages, developing common practices for applications, and exploring alternative containerisation software.},
  doi = {10.48550/arxiv.2001.10641},
  eprint = {2001.10641},
  journal = {arXiv},
  url = {https://api.semanticscholar.org/CorpusId:260446790},
  volume = {abs/2001.10641},
}

@article{Nst2020TheRPA,
  title = {The Rockerverse: Packages and Applications for Containerisation with R},
  author = {Daniel Nüst and Dirk Eddelbuettel and Dom Bennett and Robrecht Cannoodt and D. Clark and Gergely Daróczi and Mark Edmondson and Colin Fay and Ellis Hughes and S. Lopp and B. Marwick and Heather Nolis and Jacqueline Nolis and Hong-Sain Ooi and Karthik Ram and Noam Ross and Lori A. Shepherd and Nitesh Turaga and C. Willis and Nan Xiao and Charlotte Van Petegem},
  year = {2020},
  abstract = {The Rocker Project provides widely used Docker images for R across different application scenarios. This article surveys downstream projects that build upon the Rocker Project images and presents the current state of R packages for managing Docker images and controlling containers. These use cases cover diverse topics such as package development, reproducible research, collaborative work, cloud-based data processing, and production deployment of services. The variety of applications demonstrates the power of the Rocker Project specifically and containerisation in general. Across the diverse ways to use containers, we identified common themes: reproducible environments, scalability and efficiency, and portability across clouds. We conclude that the current growth and diversification of use cases is likely to continue its positive impact, but see the need for consolidating the Rockerverse ecosystem of packages, developing common practices for applications, and exploring alternative containerisation software.},
  doi = {10.32614/RJ-2020-007},
  journal = {The R Journal},
  url = {https://api.semanticscholar.org/CorpusId:210942882},
  volume = {12},
  pages = {437},
}

@article{ClyburneSherin2019ComputationalRVA,
  title = {Computational Reproducibility via Containers in Psychology},
  author = {April Clyburne-Sherin and Xu Fei and S. Green},
  year = {2019},
  journal = {Meta-Psychology},
  url = {https://pdfs.semanticscholar.org/e650/c2456467374ca930bb537b2d8f6aee9ef950.pdf},
}

@article{Barba2018PraxisORA,
  title = {Praxis of Reproducible Computational Science},
  author = {L. Barba},
  year = {2018},
  journal = {Computing in Science & Engineering},
  journal_impact_factor = {1.8},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8661798},
  volume = {21},
  pages = {73-78},
}

@article{Smith2020_NeuralAnalysis,
  title = {Advanced Neural Signal Processing Techniques for Electrophysiology},
  author = {Smith, John and Doe, Jane},
  year = {2020},
  abstract = {A comprehensive review of modern signal processing methods for neural data analysis.},
  doi = {10.1016/j.jneumeth.2020.108123},
  journal = {Journal of Neuroscience Methods},
  journal_impact_factor = {2.7},
  pages = {108-123},
  volume = {345},
}

@article{Johnson2019_SpectralAnalysis,
  title = {Spectral Analysis Methods for Time-Series Neural Data},
  author = {Johnson, Michael and Williams, Sarah},
  year = {2019},
  doi = {10.1007/s10827-019-00712-4},
  journal = {Computational Neuroscience},
  number = {3},
  pages = {234-256},
  volume = {42},
}

@article{Chen2021_MachineLearning,
  title = {Machine Learning Approaches for Neural Data Classification},
  author = {Takuya Isomura and Taro Toyoizumi},
  year = {2021},
  abstract = {<jats:p>For many years, a combination of principal component analysis (PCA) and independent component analysis (ICA) has been used for blind source separation (BSS). However, it remains unclear why these linear methods work well with real-world data that involve nonlinear source mixtures. This work theoretically validates that a cascade of linear PCA and ICA can solve a nonlinear BSS problem accurately—when the sensory inputs are generated from hidden sources via nonlinear mappings with sufficient dimensionality. Our proposed theorem, termed the asymptotic linearization theorem, theoretically guarantees that applying linear PCA to the inputs can reliably extract a subspace spanned by the linear projections from every hidden source as the major components—and thus projecting the inputs onto their major eigenspace can effectively recover a linear transformation of the hidden sources. Then subsequent application of linear ICA can separate all the true independent hidden sources accurately. Zero-element-wise-error nonlinear BSS is asymptotically attained when the source dimensionality is large and the input dimensionality is sufficiently larger than the source dimensionality. Our proposed theorem is validated analytically and numerically. Moreover, the same computation can be performed by using Hebbian-like plasticity rules, implying the biological plausibility of this nonlinear BSS strategy. Our results highlight the utility of linear PCA and ICA for accurately and reliably recovering nonlinearly mixed sources and suggest the importance of employing sensors with sufficient dimensionality to identify true hidden sources of real-world data.</jats:p>},
  doi = {10.1162/neco_a_01378},
  pmid = {34496387},
  eprint = {1808.00668},
  journal = {Neural Computation},
  volume = {33},
  citation_count = {6},
  journal_impact_factor = {2.7},
  number = {5},
  pages = {1234-1267},
}

@inproceedings{Brown2018_DeepLearning,
  title = {Deep Learning for Neural Signal Decoding},
  author = {Brown, Robert and Taylor, Emily},
  year = {2018},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {5678-5689},
  publisher = {NeurIPS},
}

@article{YourName2023_NovelMethod_duplicate,
  title = {A Novel Method for Neural Data Analysis},
  author = {Your-Name, First and Collaborator, Second and Your-Name, F. and Collaborator, S.},
  year = {2023},
  abstract = {We present a novel computational method for analyzing neural time-series data.},
  doi = {10.1038/s41598-023-12345-6},
  journal = {Scientific Reports},
  journal_impact_factor = {3.8},
  note = {This is a duplicate entry to demonstrate deduplication by DOI},
  pages = {12345},
  volume = {13},
}

@article{YourName2022_PreviousWork,
  title = {Foundations of Neural Signal Processing},
  author = {Your-Name, First and Advisor, Principal},
  year = {2022},
  doi = {10.1152/jn.00123.2022},
  journal = {Journal of Neurophysiology},
  journal_impact_factor = {2.1},
  number = {4},
  pages = {1567-1589},
  volume = {128},
}

@inproceedings{YourName2021_Conference,
  title = {Machine Learning Approaches for Brain Signal Classification},
  author = {Your-Name, First and Team, Member},
  year = {2021},
  booktitle = {International Conference on Machine Learning},
  organization = {ICML},
  pages = {7890-7901},
}
